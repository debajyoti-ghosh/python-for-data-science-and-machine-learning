{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Interactive Exercise\n",
    "\n",
    "In this notebook, you'll practice scraping a webpage using **Beautiful Soup**, extracting links and text, and saving results to a file.\n",
    "\n",
    "Each step includes a **collapsible hint** to guide you. At the end, a collapsed solution is provided for self-checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "Import the libraries needed for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f8899",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "You need `BeautifulSoup` from `bs4`, `urllib.request` to open URLs, and `re` for regular expressions.\n",
    "Optionally, `IPython.display` can be used to display HTML.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ca41b",
   "metadata": {},
   "source": [
    "## Step 2: Fetch and Parse Webpage\n",
    "\n",
    "Open the URL `https://analytics.usa.gov` and parse it using BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c12d4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `urllib.request.urlopen(url).read()` to get the HTML content.\n",
    "Then create a BeautifulSoup object with `html.parser`.\n",
    "Check the type of the object with `type()`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fetch and parse the webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248320b9",
   "metadata": {},
   "source": [
    "## Step 3: Inspect HTML\n",
    "\n",
    "Print the first 100 characters of the prettified HTML to get a sense of the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe767f3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `soup.prettify()` and slice the string `[:100]` to view the first 100 characters.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa065eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print first 100 characters of prettified HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1c82f",
   "metadata": {},
   "source": [
    "## Step 4: Extract All Links\n",
    "\n",
    "Use a loop to find all `<a>` tags and print their `href` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb4fac",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `soup.find_all('a')` and `.get('href')` inside a for loop.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06439ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract all links and print them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02085b5e",
   "metadata": {},
   "source": [
    "## Step 5: Extract Webpage Text\n",
    "\n",
    "Retrieve the entire textual content of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db503f3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use the `.get_text()` method of the BeautifulSoup object.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the full text of the webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d6291",
   "metadata": {},
   "source": [
    "## Step 6: Filter Links Starting with HTTP\n",
    "\n",
    "Use a regular expression to extract only external links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895e2b6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `attrs={'href': re.compile('^http')}` in `find_all` to filter links starting with 'http'.\n",
    "Loop through the results and print each link.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract and print links starting with http"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02e177",
   "metadata": {},
   "source": [
    "## Step 7: Save Links to a File\n",
    "\n",
    "Write the extracted links to a text file `parsed_data.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062196b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Open a file in write mode (`'w'`) and iterate over the filtered links.\n",
    "Convert each link to string and write it to the file with a newline (`\\n`).\n",
    "Don't forget to close the file at the end.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save links to parsed_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions (Collapsed)\n",
    "\n",
    "<details>\n",
    "<summary>Click to view solutions</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Import Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Step 2: Fetch and Parse Webpage\n",
    "r = urllib.request.urlopen('https://analytics.usa.gov').read()\n",
    "soup = BeautifulSoup(r, 'html.parser')\n",
    "type(soup)\n",
    "\n",
    "# Step 3: Inspect HTML\n",
    "print(soup.prettify()[:100])\n",
    "\n",
    "# Step 4: Extract All Links\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "\n",
    "# Step 5: Extract Webpage Text\n",
    "print(soup.get_text())\n",
    "\n",
    "# Step 6: Filter Links Starting with HTTP\n",
    "for link in soup.find_all('a', attrs={'href': re.compile('^http')}):\n",
    "    print(link)\n",
    "\n",
    "# Step 7: Save Links to a File\n",
    "with open('parsed_data.txt', 'w') as file:\n",
    "    for link in soup.find_all('a', attrs={'href': re.compile('^http')}):\n",
    "        file.write(str(link) + '\\n')\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
