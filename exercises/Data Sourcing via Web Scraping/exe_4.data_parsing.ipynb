{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with BeautifulSoup - Interactive Exercises\n",
    "\n",
    "In this notebook, you'll learn how to parse HTML using **BeautifulSoup**, extract text, and search using different filters. Each exercise has a hint you can expand, and a final cell contains solutions for self-checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Import Libraries\n",
    "\n",
    "Import the required libraries for web scraping:\n",
    "- `BeautifulSoup` from `bs4`\n",
    "- `urllib.request`\n",
    "- `re` for regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Import required libraries\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use:\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Load HTML from a URL\n",
    "\n",
    "Use `urllib.request.urlopen()` to fetch HTML from the URL `https://raw.githubusercontent.com/BigDataGal/Data-Mania-Demos/master/IoT-2018.html`. Store the HTML content in a variable called `html`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Fetch HTML content and store in 'html'\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use a `with` statement:\n",
    "```python\n",
    "url = 'https://raw.githubusercontent.com/BigDataGal/Data-Mania-Demos/master/IoT-2018.html'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    html = response.read()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Create a BeautifulSoup object\n",
    "\n",
    "Create a `soup` object by parsing the HTML using the `html.parser`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Create BeautifulSoup object\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "```python\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(type(soup))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Prettify HTML\n",
    "\n",
    "Print the first 100 characters of a prettified version of the HTML using `soup.prettify()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Print first 100 characters of prettified HTML\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "```python\n",
    "print(soup.prettify()[0:100])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Extract text from HTML\n",
    "\n",
    "Use the `get_text()` method to extract all text content from the HTML and store it in `text_only`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Extract text content\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "```python\n",
    "text_only = soup.get_text()\n",
    "print(text_only)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Searching and retrieving data\n",
    "\n",
    "Use `soup.find_all()` to search HTML elements. Practice the following:\n",
    "1. Find all `<li>` tags.\n",
    "2. Find tags with id=`link 7`.\n",
    "3. Find all `<ol>` tags.\n",
    "4. Find both `<ol>` and `<b>` tags.\n",
    "5. Find all tags that contain letter 't' using regular expressions.\n",
    "6. Find all tags using Boolean `True`.\n",
    "7. Print all hyperlinks (`href`) from `<a>` tags.\n",
    "8. Find all strings containing 'data' using regex."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Use find_all() for the exercises above\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Example code snippets for each task:\n",
    "```python\n",
    "# 1. Find all <li> tags\n",
    "soup.find_all('li')\n",
    "\n",
    "# 2. Find tags with id='link 7'\n",
    "soup.find_all(id='link 7')\n",
    "\n",
    "# 3. Find all <ol> tags\n",
    "soup.find_all('ol')\n",
    "\n",
    "# 4. Find both <ol> and <b> tags\n",
    "soup.find_all(['ol','b'])\n",
    "\n",
    "# 5. Regex: tags containing 't'\n",
    "import re\n",
    "pattern = re.compile('t')\n",
    "for tag in soup.find_all(pattern):\n",
    "    print(tag.name)\n",
    "\n",
    "# 6. All tags using True\n",
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)\n",
    "\n",
    "# 7. Print hrefs\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "\n",
    "# 8. Strings containing 'data'\n",
    "soup.find_all(string=re.compile('data'))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions (Collapsed)\n",
    "\n",
    "<details>\n",
    "<summary>Click to view solutions</summary>\n",
    "\n",
    "```python\n",
    "# Exercise 1\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# Exercise 2\n",
    "url = 'https://raw.githubusercontent.com/BigDataGal/Data-Mania-Demos/master/IoT-2018.html'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    html = response.read()\n",
    "\n",
    "# Exercise 3\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(type(soup))\n",
    "\n",
    "# Exercise 4\n",
    "print(soup.prettify()[0:100])\n",
    "\n",
    "# Exercise 5\n",
    "text_only = soup.get_text()\n",
    "print(text_only)\n",
    "\n",
    "# Exercise 6\n",
    "soup.find_all('li')\n",
    "soup.find_all(id='link 7')\n",
    "soup.find_all('ol')\n",
    "soup.find_all(['ol','b'])\n",
    "pattern = re.compile('t')\n",
    "for tag in soup.find_all(pattern):\n",
    "    print(tag.name)\n",
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "soup.find_all(string=re.compile('data'))\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
