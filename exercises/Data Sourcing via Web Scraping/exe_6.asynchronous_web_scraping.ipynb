{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caba9b2",
   "metadata": {},
   "source": [
    "# Interactive Exercise: Asynchronous Web Scraping\n",
    "In this notebook, you will practice **asynchronous web scraping** using Python. You will write code to fetch multiple URLs concurrently, parse the HTML, and save links to a CSV file.\n",
    "Each step includes a **hint**. After completing all exercises, check the **collapsed solution** section at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122e240",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Run `pip install aiohttp asyncio nest-asyncio beautifulsoup4` to ensure all necessary libraries are installed.\n",
    "</details>\n",
    "### Exercise:\n",
    "Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f02e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install packages\n",
    "!pip install aiohttp asyncio nest-asyncio beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8aeff",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Apply `nest_asyncio`\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Import `aiohttp`, `asyncio`, `BeautifulSoup`, `csv`, `re`, and `nest_asyncio`. Then call `nest_asyncio.apply()` to allow nested event loops in Jupyter.\n",
    "</details>\n",
    "### Exercise:\n",
    "Write the import statements and apply nest_asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984de306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import libraries and apply nest_asyncio\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07334b3",
   "metadata": {},
   "source": [
    "## Step 3: Define Asynchronous Function to Parse HTML and Save Links\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `async def` to define the function.\n",
    "Use BeautifulSoup to parse the HTML.\n",
    "Use `soup.findAll('a', attrs={'href': re.compile('^http')})` to find all links starting with http.\n",
    "Use `csv.writer` to write links to a CSV file.\n",
    "</details>\n",
    "### Exercise:\n",
    "Define an asynchronous function `scrap_and_save_links(text)` that extracts all links from the HTML `text` and saves them in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define scrap_and_save_links\n",
    "async def scrap_and_save_links(text):\n",
    "    # parse HTML\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    # open CSV file\n",
    "    with open('csv_file.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        # find all links starting with http\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile('^http')}):\n",
    "            url = link.get('href')\n",
    "            writer.writerow([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497aec99",
   "metadata": {},
   "source": [
    "## Step 4: Define Asynchronous Fetch Function\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `async with session.get(url) as response` to fetch the webpage.\n",
    "Use `await response.text()` to get the HTML content.\n",
    "Create a task with `asyncio.create_task(scrap_and_save_links(text))` and await it.\n",
    "</details>\n",
    "### Exercise:\n",
    "Write `async def fetch(session, url)` to fetch HTML content from a URL and call `scrap_and_save_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define fetch function\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            text = await response.text()\n",
    "            task = asyncio.create_task(scrap_and_save_links(text))\n",
    "            await task\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a04815",
   "metadata": {},
   "source": [
    "## Step 5: Scrape Multiple URLs Concurrently\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Create a list of tasks for each URL and use `await asyncio.gather(*tasks)`.\n",
    "Use `aiohttp.ClientSession()` to maintain a session for all requests.\n",
    "</details>\n",
    "### Exercise:\n",
    "Define `async def scrap(urls)` to concurrently scrape a list of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a751e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define scrap function\n",
    "async def scrap(urls):\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for url in urls:\n",
    "            tasks.append(fetch(session, url))\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d23260",
   "metadata": {},
   "source": [
    "## Step 6: Run the Asynchronous Scraper\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Provide a list of URLs and call `asyncio.run(scrap(urls))` to run the scraper.\n",
    "</details>\n",
    "### Exercise:\n",
    "Run the scraper on a list of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run scraper\n",
    "urls = [\n",
    "    'https://analytics.usa.gov/',\n",
    "    'https://www.python.org/',\n",
    "    'https://www.linkedin.com/'\n",
    "]\n",
    "\n",
    "asyncio.run(scrap(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84880fb",
   "metadata": {},
   "source": [
    "## âœ… Step 7: Self-Check Solutions\n",
    "<details>\n",
    "<summary>Click to view full solutions</summary>\n",
    "```python\n",
    "# Step 2: Imports\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Step 3: scrap_and_save_links\n",
    "async def scrap_and_save_links(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    with open('csv_file.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile('^http')}):\n",
    "            writer.writerow([link.get('href')])\n",
    "\n",
    "# Step 4: fetch\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            text = await response.text()\n",
    "            task = asyncio.create_task(scrap_and_save_links(text))\n",
    "            await task\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {str(e)}\")\n",
    "\n",
    "# Step 5: scrap\n",
    "async def scrap(urls):\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for url in urls:\n",
    "            tasks.append(fetch(session, url))\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "# Step 6: Run\n",
    "urls = ['https://analytics.usa.gov/', 'https://www.python.org/', 'https://www.linkedin.com/']\n",
    "asyncio.run(scrap(urls))\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
