{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c29c35",
   "metadata": {},
   "source": [
    "# Web Scraping in Practice\n",
    "\n",
    "In this notebook, we will learn how to scrape web pages using **Beautiful Soup**, retrieve links, extract text, and save results to an external file. We'll demonstrate scraping data from [analytics.usa.gov](https://analytics.usa.gov)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03f06d",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We'll start by importing the required libraries:\n",
    "- `BeautifulSoup` from `bs4` for parsing HTML\n",
    "- `urllib.request` to open URLs\n",
    "- `re` for regular expressions\n",
    "- `IPython.display` for displaying HTML content in notebook (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b202cc",
   "metadata": {},
   "source": [
    "## Step 2: Fetch and Parse Webpage\n",
    "\n",
    "We'll open the URL using `urllib.request.urlopen` and read the content. Then, we'll parse it using `BeautifulSoup` with the `html.parser`.\n",
    "\n",
    "We also check the type of the parsed object to ensure it is a `BeautifulSoup` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the URL and read content\n",
    "r = urllib.request.urlopen('https://analytics.usa.gov').read()\n",
    "\n",
    "# Parse HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(r, 'html.parser')\n",
    "\n",
    "# Check type of the object\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34caee79",
   "metadata": {},
   "source": [
    "## Step 3: Inspecting the Parsed HTML\n",
    "\n",
    "We can print the HTML in a readable format using `prettify()`. We'll look at only the first 100 characters to avoid too much output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe8c1f",
   "metadata": {},
   "source": [
    "## Step 4: Extract All Links\n",
    "\n",
    "We use `find_all('a')` to find all anchor (`<a>`) tags and then extract their `href` attributes using `.get('href')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a703a11",
   "metadata": {},
   "source": [
    "## Step 5: Extract All Text\n",
    "\n",
    "We can also get the entire textual content of the webpage using `.get_text()`. This is useful if we want the raw text without HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee27724",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013fdca",
   "metadata": {},
   "source": [
    "## Step 6: Prettify and Inspect Larger Portion\n",
    "\n",
    "We can also prettify and view the first 1000 characters of the HTML for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0348f697",
   "metadata": {},
   "source": [
    "## Step 7: Extract Links Matching a Pattern\n",
    "\n",
    "To filter links starting with `http`, we can use `attrs={'href': re.compile('^http')}`. This is useful to extract only external URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a', attrs={'href': re.compile('^http')}):\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1547a",
   "metadata": {},
   "source": [
    "We can also inspect the type of the object returned by `find_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1361b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14099e1b",
   "metadata": {},
   "source": [
    "## Step 8: Save Extracted Links to a File\n",
    "\n",
    "We can write the filtered links to a text file for future use. Here, we convert each link to a string and write it to `parsed_data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a39c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('parsed_data.txt', 'w')\n",
    "for link in soup.find_all('a', attrs={'href': re.compile('^http')}):\n",
    "    soup_link = str(link)\n",
    "    print(soup_link)\n",
    "    file.write(soup_link + '\\n')  # Add newline for readability\n",
    "file.flush()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb916d",
   "metadata": {},
   "source": [
    "## Step 9: Check Current Directory\n",
    "\n",
    "Use `%pwd` to see where your output file has been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8b7a2",
   "metadata": {},
   "source": [
    "---\n",
    "### Summary\n",
    "- We fetched a webpage and parsed it using Beautiful Soup.\n",
    "- Extracted all links and filtered by pattern using regex.\n",
    "- Extracted the text content of the webpage.\n",
    "- Saved the results to an external file for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
