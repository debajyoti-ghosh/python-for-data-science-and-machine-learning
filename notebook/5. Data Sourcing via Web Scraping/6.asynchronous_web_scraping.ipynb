{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Web Scraping\n",
    "In this notebook, we will explore **asynchronous web scraping** using Python. Unlike synchronous scraping, which waits for each request to complete, asynchronous scraping allows multiple requests to happen simultaneously, making the process faster and more efficient.\n",
    "\n",
    "We will use the following libraries:\n",
    "- `aiohttp` for making asynchronous HTTP requests\n",
    "- `asyncio` for writing concurrent code\n",
    "- `BeautifulSoup` for parsing HTML\n",
    "- `csv` for saving extracted links\n",
    "- `re` for filtering links with regular expressions\n",
    "- `nest_asyncio` to handle nested event loops in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Run `pip install aiohttp asyncio nest-asyncio beautifulsoup4` to make sure all libraries are installed.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install aiohttp asyncio nest-asyncio beautifulsoup4"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Import `aiohttp`, `asyncio`, `BeautifulSoup`, `csv`, `re`, and `nest_asyncio`. Apply `nest_asyncio.apply()` to enable nested event loops in Jupyter Notebook.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Asynchronous Function to Parse and Save Links\n",
    "We define an **asynchronous function** that:\n",
    "1. Parses the HTML content using BeautifulSoup\n",
    "2. Extracts all links that start with `http`\n",
    "3. Saves them into a CSV file\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `async def` to define asynchronous functions. Use `soup.findAll('a', attrs={'href': re.compile('^http')})` to extract all valid links.\n",
    "Use `csv.writer` to write links to a CSV file.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "async def scrap_and_save_links(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    with open('csv_file.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile('^http')}):\n",
    "            url = link.get('href')\n",
    "            writer.writerow([url])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fetch HTML Content Asynchronously\n",
    "This function fetches the HTML of a webpage asynchronously and passes it to `scrap_and_save_links`.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `async with session.get(url) as response` to make an asynchronous GET request.\n",
    "Use `await response.text()` to get the HTML content.\n",
    "Create a task with `asyncio.create_task(scrap_and_save_links(text))` and await it.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            text = await response.text()\n",
    "            task = asyncio.create_task(scrap_and_save_links(text))\n",
    "            await task\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {str(e)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Scrape Multiple URLs Concurrently\n",
    "This function schedules multiple fetch tasks concurrently using `asyncio.gather`.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Create a list of tasks and pass them to `await asyncio.gather(*tasks)`.\n",
    "Use `aiohttp.ClientSession()` to maintain session for all requests.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "async def scrap(urls):\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for url in urls:\n",
    "            tasks.append(fetch(session, url))\n",
    "        await asyncio.gather(*tasks)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the Asynchronous Scraper\n",
    "We provide a list of URLs to scrape and run the asynchronous scraper.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Use `asyncio.run(scrap(urls))` to run the asynchronous event loop.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "urls = [\n",
    "    'https://analytics.usa.gov/',\n",
    "    'https://www.python.org/',\n",
    "    'https://www.linkedin.com/'\n",
    "]\n",
    "\n",
    "asyncio.run(scrap(urls))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "- We learned how **asynchronous scraping** can speed up web data extraction.\n",
    "- Used `aiohttp` and `asyncio` for concurrent HTTP requests.\n",
    "- Parsed HTML with BeautifulSoup and saved links to a CSV file.\n",
    "- Handled Jupyter Notebook's event loop using `nest_asyncio`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
